{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f233670d",
   "metadata": {},
   "source": [
    "# Construct Zarr Chips\n",
    "\n",
    "This notebook constructs zarr chips from the raw (tile based sentinel 2) data on the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "505e69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40556a99",
   "metadata": {},
   "source": [
    "# Open all Samples and Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e270594",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gpd.read_parquet(\"data/samples.parquet\")\n",
    "labels = pd.read_parquet(\"data/labels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127acc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = {\n",
    "    \"blue\": \"B02_10m\",\n",
    "    \"green\": \"B03_10m\",\n",
    "    \"red\": \"B04_10m\",\n",
    "    \"nir\": \"B08_10m\",\n",
    "    \"rededge1\": \"B05_20m\",\n",
    "    \"rededge2\": \"B06_20m\",\n",
    "    \"rededge3\": \"B07_20m\",\n",
    "    \"nir08\": \"B8A_20m\",\n",
    "    \"swir16\": \"B11_20m\",\n",
    "    \"swir22\": \"B12_20m\",\n",
    "    \"scl\": \"SCL_20m\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a8c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_band_files(product_folder):\n",
    "    # Parses the manifest.SAFE file for elements\n",
    "    tree = ET.parse(product_folder / \"manifest.safe\")\n",
    "    root = tree.getroot()\n",
    "\n",
    "    data_objects = root.find(\"dataObjectSection\").findall(\"dataObject\")\n",
    "\n",
    "    band_files = {}\n",
    "\n",
    "    for data_object in data_objects:\n",
    "        file_location = (\n",
    "            data_object.find(\"byteStream\").find(\"fileLocation\").attrib[\"href\"]\n",
    "        )  # The path to that file\n",
    "\n",
    "        # Searches for a match of the band name within the ID of the object\n",
    "        for band in bands.values():\n",
    "            if band in file_location:\n",
    "                band_files[band] = product_folder / file_location\n",
    "\n",
    "    return band_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026c8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_zarr_chip(ds, geom, sample_id):\n",
    "    # Find nearest x/y indices\n",
    "    x_idx = np.abs(ds.x - geom.x).argmin().item()\n",
    "    y_idx = np.abs(ds.y - geom.y).argmin().item()\n",
    "\n",
    "    # Define window size\n",
    "    full_size = 128\n",
    "    x_start = max(0, x_idx - (full_size // 2))\n",
    "    x_end = x_start + full_size\n",
    "    y_start = max(0, y_idx - (full_size // 2))\n",
    "    y_end = y_start + full_size\n",
    "\n",
    "    # Subset dataset\n",
    "    subset = ds.isel(x=slice(x_start, x_end), y=slice(y_start, y_end))\n",
    "    data_mask = (subset[\"SCL\"] == 0).any(dim=(\"y\", \"x\"))\n",
    "    if data_mask:\n",
    "        warnings.warn(\n",
    "            f\"Sample {sample_id} contains no valid data (SCL == 0). Skipping.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    get_data = subset.astype(\"uint16\")\n",
    "\n",
    "    # There's a bug with xarray doing over-eager conversion of timestamps (see https://github.com/pydata/xarray/issues/3942)\n",
    "    # so we need to specify a time encoding\n",
    "    encoding = {\n",
    "        \"time\": {\n",
    "            \"units\": \"seconds since 2015-01-01\",\n",
    "            \"calendar\": \"standard\",\n",
    "            \"dtype\": \"int64\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # This is to remove scale and offset, it messes with appending correct dtypes\n",
    "    for data_var in get_data.data_vars:\n",
    "        encoding[data_var] = {\"dtype\": \"uint16\"}\n",
    "        get_data[data_var].attrs = {}\n",
    "\n",
    "    # Write to zarr\n",
    "    zarr_path = f\"data/chips/{sample_id}.zarr\"\n",
    "    if not os.path.exists(zarr_path):\n",
    "        # First time: initialize the zarr\n",
    "        get_data.to_zarr(zarr_path, mode=\"w\", zarr_format=3, encoding=encoding)\n",
    "    else:\n",
    "        # Next times: append along time\n",
    "        get_data.drop_attrs().to_zarr(\n",
    "            zarr_path, append_dim=\"time\", mode=\"a\", zarr_format=3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f4c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentinel2_bands(band_files, resolution=\"10m\"):\n",
    "    \"\"\"Load Sentinel-2 bands into a properly structured xarray Dataset\"\"\"\n",
    "\n",
    "    # Filter bands by resolution\n",
    "    filtered_bands = {k: v for k, v in band_files.items() if k.endswith(resolution)}\n",
    "\n",
    "    # Load each band\n",
    "    band_arrays = {}\n",
    "    for band_name, file_path in filtered_bands.items():\n",
    "        # Open with rioxarray to preserve spatial reference\n",
    "        da = rioxarray.open_rasterio(file_path, chunks=True, mask_and_scale=False)\n",
    "        da = da.squeeze().drop_vars(\"band\")  # Remove band dimension (it's singular)\n",
    "\n",
    "        # Clean band name (B02, B03, etc.)\n",
    "        clean_name = band_name.split(\"_\")[0]\n",
    "        band_arrays[clean_name] = da\n",
    "\n",
    "    # Create dataset\n",
    "    ds = xr.Dataset(band_arrays)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818541a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_acquisitions(acquisitions, samples_df):\n",
    "    # try to load zarr from last sample_id and check which acquisitions have already been added\n",
    "    last_sample = samples_df.iloc[-1]\n",
    "    zarr_path = f\"data/chips/{last_sample.sample_id}.zarr\"\n",
    "    if not os.path.exists(zarr_path):\n",
    "        return acquisitions\n",
    "    # getting last added acquisition and adding some tolerance due to smaller precision of time stored in zarr\n",
    "    last_added_acquisition = pd.Timestamp(\n",
    "        xr.open_zarr(zarr_path).time.values[-1]\n",
    "    ) + pd.Timedelta(seconds=10)\n",
    "    return [\n",
    "        acquisition\n",
    "        for acquisition in acquisitions\n",
    "        if pd.Timestamp(acquisition.stem.split(\"_\")[1]) > last_added_acquisition\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce0f9a",
   "metadata": {},
   "source": [
    "# FNEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b91de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['33UUS', '33UVS', '32UMU', '32TMT']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnews_samples = samples.query(\n",
    "    \"dataset=='Evoland' and source=='Regional Forestry Departments'\"\n",
    ")\n",
    "tiles = list(fnews_samples[\"s2_tile\"].unique())[3:]\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "877cb7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>original_sample_id</th>\n",
       "      <th>interpreter</th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "      <th>source_description</th>\n",
       "      <th>s2_tile</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>cluster_description</th>\n",
       "      <th>comment</th>\n",
       "      <th>confidence</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>606</td>\n",
       "      <td>612</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>32UNA</td>\n",
       "      <td>198.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>1, 2019/08/18</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (9.05234 50.07864)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>607</td>\n",
       "      <td>616</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>32UNA</td>\n",
       "      <td>209.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>1, 2019/08/18</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (9.31017 50.11753)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>608</td>\n",
       "      <td>624</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>32UNA</td>\n",
       "      <td>208.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>1, 2019/08/18</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (9.0414 50.07218)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>609</td>\n",
       "      <td>625</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>32UNA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>1, 2019/08/18</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (9.04996 50.06712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>610</td>\n",
       "      <td>631</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>32UNA</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>1, 2019/08/18</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (9.02246 50.08142)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>1001</td>\n",
       "      <td>1263</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>33UVS</td>\n",
       "      <td>421.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>22</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (14.08833 50.99596)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>1002</td>\n",
       "      <td>1264</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>33UVS</td>\n",
       "      <td>420.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>22</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (14.10273 51.0011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>1003</td>\n",
       "      <td>1265</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>33UVS</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td></td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (14.11205 51.00166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>1004</td>\n",
       "      <td>1266</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>33UVS</td>\n",
       "      <td>419.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>22</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (14.10425 51.00364)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>1005</td>\n",
       "      <td>1267</td>\n",
       "      <td>pum</td>\n",
       "      <td>Evoland</td>\n",
       "      <td>Regional Forestry Departments</td>\n",
       "      <td>FNews Project, German Forestry Departmetns Sou...</td>\n",
       "      <td>33UVS</td>\n",
       "      <td>440.0</td>\n",
       "      <td>Damage polygons</td>\n",
       "      <td>22</td>\n",
       "      <td>high</td>\n",
       "      <td>POINT (14.11987 51.02323)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_id  original_sample_id interpreter  dataset  \\\n",
       "606         606                 612         pum  Evoland   \n",
       "607         607                 616         pum  Evoland   \n",
       "608         608                 624         pum  Evoland   \n",
       "609         609                 625         pum  Evoland   \n",
       "610         610                 631         pum  Evoland   \n",
       "...         ...                 ...         ...      ...   \n",
       "1001       1001                1263         pum  Evoland   \n",
       "1002       1002                1264         pum  Evoland   \n",
       "1003       1003                1265         pum  Evoland   \n",
       "1004       1004                1266         pum  Evoland   \n",
       "1005       1005                1267         pum  Evoland   \n",
       "\n",
       "                             source  \\\n",
       "606   Regional Forestry Departments   \n",
       "607   Regional Forestry Departments   \n",
       "608   Regional Forestry Departments   \n",
       "609   Regional Forestry Departments   \n",
       "610   Regional Forestry Departments   \n",
       "...                             ...   \n",
       "1001  Regional Forestry Departments   \n",
       "1002  Regional Forestry Departments   \n",
       "1003  Regional Forestry Departments   \n",
       "1004  Regional Forestry Departments   \n",
       "1005  Regional Forestry Departments   \n",
       "\n",
       "                                     source_description s2_tile cluster_id  \\\n",
       "606   FNews Project, German Forestry Departmetns Sou...   32UNA      198.0   \n",
       "607   FNews Project, German Forestry Departmetns Sou...   32UNA      209.0   \n",
       "608   FNews Project, German Forestry Departmetns Sou...   32UNA      208.0   \n",
       "609   FNews Project, German Forestry Departmetns Sou...   32UNA      200.0   \n",
       "610   FNews Project, German Forestry Departmetns Sou...   32UNA      202.0   \n",
       "...                                                 ...     ...        ...   \n",
       "1001  FNews Project, German Forestry Departmetns Sou...   33UVS      421.0   \n",
       "1002  FNews Project, German Forestry Departmetns Sou...   33UVS      420.0   \n",
       "1003  FNews Project, German Forestry Departmetns Sou...   33UVS       <NA>   \n",
       "1004  FNews Project, German Forestry Departmetns Sou...   33UVS      419.0   \n",
       "1005  FNews Project, German Forestry Departmetns Sou...   33UVS      440.0   \n",
       "\n",
       "     cluster_description        comment confidence                   geometry  \n",
       "606      Damage polygons  1, 2019/08/18       high   POINT (9.05234 50.07864)  \n",
       "607      Damage polygons  1, 2019/08/18       high   POINT (9.31017 50.11753)  \n",
       "608      Damage polygons  1, 2019/08/18       high    POINT (9.0414 50.07218)  \n",
       "609      Damage polygons  1, 2019/08/18       high   POINT (9.04996 50.06712)  \n",
       "610      Damage polygons  1, 2019/08/18       high   POINT (9.02246 50.08142)  \n",
       "...                  ...            ...        ...                        ...  \n",
       "1001     Damage polygons             22       high  POINT (14.08833 50.99596)  \n",
       "1002     Damage polygons             22       high   POINT (14.10273 51.0011)  \n",
       "1003     Damage polygons                      high  POINT (14.11205 51.00166)  \n",
       "1004     Damage polygons             22       high  POINT (14.10425 51.00364)  \n",
       "1005     Damage polygons             22       high  POINT (14.11987 51.02323)  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnews_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5360069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33UUS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33UVS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [4:00:59<00:00, 34.35s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32UMU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/469 [02:03<3:13:20, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maniftest file missing for tile \\\\digs110\\FER\\fnews\\RasterData\\L2A\\32UMU\\2015\\L2A_20150908T103707_108A_32UMU.SAFE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/469 [05:46<3:07:25, 24.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maniftest file missing for tile \\\\digs110\\FER\\fnews\\RasterData\\L2A\\32UMU\\2016\\L2A_20160205T103556_108A_32UMU.SAFE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 37/469 [14:23<3:18:07, 27.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maniftest file missing for tile \\\\digs110\\FER\\fnews\\RasterData\\L2A\\32UMU\\2016\\L2A_20160929T102344_065A_32UMU.SAFE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 40/469 [15:24<2:52:08, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maniftest file missing for tile \\\\digs110\\FER\\fnews\\RasterData\\L2A\\32UMU\\2016\\L2A_20161022T103357_108A_32UMU.SAFE\n",
      "maniftest file missing for tile \\\\digs110\\FER\\fnews\\RasterData\\L2A\\32UMU\\2016\\L2A_20161101T103156_108A_32UMU.SAFE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [3:09:34<00:00, 24.25s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32TMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 516/516 [3:49:35<00:00, 26.70s/it]  \n"
     ]
    }
   ],
   "source": [
    "for tile in tiles:\n",
    "    print(tile)\n",
    "    acquisitions = list(\n",
    "        Path(f\"//digs110/FER/fnews/RasterData/L2A/{tile}\").glob(\"*/*.SAFE\")\n",
    "    )\n",
    "    fnews_tile_reprojected = fnews_samples.query(\"s2_tile==@tile\").to_crs(\n",
    "        f\"EPSG:326{tile[0:2]}\"\n",
    "    )\n",
    "    filtered_acquisitions = filter_acquisitions(acquisitions, fnews_tile_reprojected)\n",
    "    for product_folder in tqdm(filtered_acquisitions):\n",
    "        timestamp = pd.Timestamp(product_folder.stem.split(\"_\")[1])\n",
    "        try:\n",
    "            band_files = get_band_files(product_folder)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"maniftest file missing for tile {product_folder}\")\n",
    "            continue\n",
    "\n",
    "        # load data\n",
    "        try:\n",
    "            ds_10m = load_sentinel2_bands(band_files, \"10m\")\n",
    "            ds_20m = (\n",
    "                load_sentinel2_bands(band_files, \"20m\")\n",
    "                .interp(\n",
    "                    x=ds_10m[\"x\"],\n",
    "                    y=ds_10m[\"y\"],\n",
    "                    method=\"nearest\",\n",
    "                    kwargs={\"fill_value\": \"extrapolate\"},\n",
    "                )\n",
    "                .astype(\"uint16\")\n",
    "            )\n",
    "        except KeyError:\n",
    "            print(f\"some bands not found in manifest for tile {product_folder}\")\n",
    "            continue\n",
    "        ds = (\n",
    "            xr.merge([ds_10m, ds_20m])\n",
    "            .expand_dims(dim=\"time\")\n",
    "            .assign_coords(time=[timestamp])\n",
    "            .compute()\n",
    "        )\n",
    "\n",
    "        # write out chips for all samples\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            _ = fnews_tile_reprojected.apply(\n",
    "                lambda geo_series: write_zarr_chip(\n",
    "                    ds, geo_series.geometry, geo_series.sample_id\n",
    "                ),\n",
    "                axis=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f73cb",
   "metadata": {},
   "source": [
    "# Evoland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20271b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30SUF', '33VVJ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evo_samples = samples.query(\n",
    "    \"dataset=='Evoland' and source != 'Regional Forestry Departments'\"\n",
    ")\n",
    "tiles = list(evo_samples[\"s2_tile\"].unique())\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc84b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30SUF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [4:24:49<00:00, 73.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33VVJ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [3:26:28<00:00, 90.42s/it] \n"
     ]
    }
   ],
   "source": [
    "for tile in tiles:\n",
    "    print(tile)\n",
    "    acquisitions = list(\n",
    "        Path(f\"//digs110/FER/EvoLand/WP2_6_CFM/RasterData/L2A/{tile}\").glob(\"*.SAFE\")\n",
    "    )\n",
    "    tiles_reprojected = evo_samples.query(\"s2_tile==@tile\").to_crs(\n",
    "        f\"EPSG:326{tile[0:2]}\"\n",
    "    )\n",
    "    filtered_acquisitions = filter_acquisitions(acquisitions, tiles_reprojected)\n",
    "    for product_folder in tqdm(filtered_acquisitions):\n",
    "        timestamp = pd.Timestamp(product_folder.stem.split(\"_\")[1])\n",
    "        try:\n",
    "            band_files = get_band_files(product_folder)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"manifest file missing for tile {product_folder}\")\n",
    "            continue\n",
    "\n",
    "        # load data\n",
    "        try:\n",
    "            ds_10m = load_sentinel2_bands(band_files, \"10m\")\n",
    "            ds_20m = (\n",
    "                load_sentinel2_bands(band_files, \"20m\")\n",
    "                .interp(\n",
    "                    x=ds_10m[\"x\"],\n",
    "                    y=ds_10m[\"y\"],\n",
    "                    method=\"nearest\",\n",
    "                    kwargs={\"fill_value\": \"extrapolate\"},\n",
    "                )\n",
    "                .astype(\"uint16\")\n",
    "            )\n",
    "        except KeyError:\n",
    "            print(f\"some bands not found in manifest for tile {product_folder}\")\n",
    "            continue\n",
    "        ds = (\n",
    "            xr.merge([ds_10m, ds_20m])\n",
    "            .expand_dims(dim=\"time\")\n",
    "            .assign_coords(time=[timestamp])\n",
    "            .compute()\n",
    "        )\n",
    "\n",
    "        # write out chips for all samples\n",
    "        with warnings.catch_warnings():\n",
    "            # filtering warning about consolidated zarr metadata\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            _ = tiles_reprojected.apply(\n",
    "                lambda geo_series: write_zarr_chip(\n",
    "                    ds, geo_series.geometry, geo_series.sample_id\n",
    "                ),\n",
    "                axis=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fba19",
   "metadata": {},
   "source": [
    "# HRVPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cf524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['29SPC',\n",
       " '29UNV',\n",
       " '30SVG',\n",
       " '30TXQ',\n",
       " '31UDR',\n",
       " '31UFS',\n",
       " '32TNS',\n",
       " '33UVS',\n",
       " '33VUF',\n",
       " '34TDS',\n",
       " '34WDB',\n",
       " '35TLG',\n",
       " '35VMJ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hrvpp_samples = samples.query(\"dataset=='HRVPP'\")\n",
    "tiles = list(hrvpp_samples[\"s2_tile\"].unique())\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e0b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hvrpp_band_files(product_folder):\n",
    "    band_files = {\"SCL_20m\": product_folder}  # Initialize with SCL_20m band\n",
    "    # Searches for a match of the band name within the ID of the object\n",
    "    for band in bands.values():\n",
    "        if band == \"SCL_20m\":\n",
    "            continue\n",
    "        band_files[band] = Path(\n",
    "            str(product_folder).replace(\n",
    "                \"SCENECLASSIFICATION_20M\", f\"TOC-{band.upper()}\"\n",
    "            )\n",
    "        )\n",
    "    return band_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38d1fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29SPC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29UNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30SVG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [3:20:34<00:00, 69.97s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30TXQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1803/1803 [19:15:37<00:00, 38.46s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31UDR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 296/1209 [3:31:03<10:51:00, 42.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msome bands not found in manifest for tile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     21\u001b[39m ds = (\n\u001b[32m     22\u001b[39m     \u001b[43mxr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds_10m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_20m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43massign_coords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# write out chips for all samples\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonas.Viehweger\\Documents\\Projects\\2025\\disturbance-agent-data\\.venv\\Lib\\site-packages\\xarray\\core\\dataset.py:715\u001b[39m, in \u001b[36mDataset.compute\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Manually trigger loading and/or computation of this dataset's data\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03mfrom disk or a remote source into memory and return a new dataset.\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[33;03mUnlike load, the original dataset is left unaltered.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    712\u001b[39m \u001b[33;03mdask.compute\u001b[39;00m\n\u001b[32m    713\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    714\u001b[39m new = \u001b[38;5;28mself\u001b[39m.copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonas.Viehweger\\Documents\\Projects\\2025\\disturbance-agent-data\\.venv\\Lib\\site-packages\\xarray\\core\\dataset.py:542\u001b[39m, in \u001b[36mDataset.load\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m chunkmanager = get_chunked_array_type(*lazy_data.values())\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, Any], ...] = \u001b[43mchunkmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mlazy_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m.variables[k].data = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonas.Viehweger\\Documents\\Projects\\2025\\disturbance-agent-data\\.venv\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:85\u001b[39m, in \u001b[36mDaskManager.compute\u001b[39m\u001b[34m(self, *data, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\n\u001b[32m     81\u001b[39m     \u001b[38;5;28mself\u001b[39m, *data: Any, **kwargs: Any\n\u001b[32m     82\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[np.ndarray[Any, _DType_co], ...]:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdask\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Jonas.Viehweger\\Documents\\Projects\\2025\\disturbance-agent-data\\.venv\\Lib\\site-packages\\dask\\base.py:681\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     expr = expr.optimize()\n\u001b[32m    679\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(flatten(expr.__dask_keys__()))\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for tile in tiles:\n",
    "    print(tile)\n",
    "    acquisitions = list(\n",
    "        Path(\"//digs110/FER/HR-VPP2/Data/TOC/v00/\").glob(\n",
    "            f\"**/*_{tile}_SCENECLASSIFICATION*.tif\"\n",
    "        )\n",
    "    )\n",
    "    tiles_reprojected = hrvpp_samples.query(\"s2_tile==@tile\").to_crs(\n",
    "        f\"EPSG:326{tile[0:2]}\"\n",
    "    )\n",
    "    filtered_acquisitions = filter_acquisitions(acquisitions, tiles_reprojected)\n",
    "    for product_folder in tqdm(filtered_acquisitions):\n",
    "        timestamp = pd.Timestamp(product_folder.stem.split(\"_\")[1])\n",
    "        band_files = get_hvrpp_band_files(product_folder)\n",
    "\n",
    "        # load data\n",
    "        try:\n",
    "            ds_10m = load_sentinel2_bands(band_files, \"10m\")\n",
    "            ds_20m = load_sentinel2_bands(band_files, \"20m\").interp(\n",
    "                x=ds_10m[\"x\"],\n",
    "                y=ds_10m[\"y\"],\n",
    "                method=\"nearest\",\n",
    "                kwargs={\"fill_value\": \"extrapolate\"},\n",
    "            )\n",
    "        except KeyError:\n",
    "            print(f\"some bands not found in manifest for tile {product_folder}\")\n",
    "            continue\n",
    "        ds = (\n",
    "            xr.merge([ds_10m, ds_20m])\n",
    "            .expand_dims(dim=\"time\")\n",
    "            .assign_coords(time=[timestamp])\n",
    "            .compute()\n",
    "        )\n",
    "\n",
    "        # write out chips for all samples\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            _ = tiles_reprojected.apply(\n",
    "                lambda geo_series: write_zarr_chip(\n",
    "                    ds, geo_series.geometry, geo_series.sample_id\n",
    "                ),\n",
    "                axis=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4e54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_zarr(zarr_id):\n",
    "    x = xr.open_zarr(\n",
    "        f\"data/chips/{zarr_id}.zarr\", mask_and_scale=False, decode_coords=\"all\"\n",
    "    )\n",
    "\n",
    "    new_size = 128\n",
    "    new_time_dim = 32\n",
    "\n",
    "    small_chip = x.isel(\n",
    "        x=slice(len(x.x) // 2 - new_size // 2, len(x.x) // 2 + new_size // 2),\n",
    "        y=slice(len(x.y) // 2 - new_size // 2, len(x.y) // 2 + new_size // 2),\n",
    "    )\n",
    "\n",
    "    # only take chips with full coverage\n",
    "    data_mask = (small_chip[\"SCL\"] == 0).any(dim=(\"y\", \"x\"))\n",
    "\n",
    "    # rechunk to 32x128x128 this results in chunks of around 1MB in uncompressed size\n",
    "    # ideally we would also shard, to reduce the number of files\n",
    "    # however I didn't get this to work yet with xarray\n",
    "    rechunked = small_chip.sel(time=~data_mask).chunk(\n",
    "        {\"time\": new_time_dim, \"y\": new_size, \"x\": new_size}\n",
    "    )\n",
    "    for var in rechunked:\n",
    "        del rechunked[var].encoding[\"chunks\"]\n",
    "    with warnings.catch_warnings():\n",
    "        # rioxarray is warning about different scales per band, did not find a way to handle this warning\n",
    "        # so we just ignore it\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"zarr\")\n",
    "        rechunked.to_zarr(f\"data/cleaned_chips/{zarr_id}.zarr\", zarr_format=3)\n",
    "    # delete the original zarr\n",
    "    shutil.rmtree(f\"data/chips/{zarr_id}.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c42d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_last_tile = \"35VMJ\"\n",
    "only_finished = hrvpp_samples.query(\"s2_tile!=@expect_last_tile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "932264dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for zarr_id in tqdm(list(only_finished.sample_id)[0:10]):\n",
    "    try:\n",
    "        clean_zarr(zarr_id)\n",
    "    except Exception:\n",
    "        print(f\"Error processing {zarr_id}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0058db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "def clean_zarr(zarr_id):\n",
    "    try:\n",
    "        x = xr.open_zarr(\n",
    "            f\"data/chips/{zarr_id}.zarr\", mask_and_scale=False, decode_coords=\"all\"\n",
    "        )\n",
    "        new_size = 128\n",
    "        new_time_dim = 32\n",
    "        small_chip = x.isel(\n",
    "            x=slice(len(x.x) // 2 - new_size // 2, len(x.x) // 2 + new_size // 2),\n",
    "            y=slice(len(x.y) // 2 - new_size // 2, len(x.y) // 2 + new_size // 2),\n",
    "        )\n",
    "        data_mask = (small_chip[\"SCL\"] == 0).any(dim=(\"y\", \"x\"))\n",
    "        rechunked = small_chip.sel(time=~data_mask).chunk(\n",
    "            {\"time\": new_time_dim, \"y\": new_size, \"x\": new_size}\n",
    "        )\n",
    "\n",
    "        for var in rechunked:\n",
    "            del rechunked[var].encoding[\"chunks\"]\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"zarr\")\n",
    "            rechunked.to_zarr(f\"data/cleaned_chips/{zarr_id}.zarr\", zarr_format=3)\n",
    "\n",
    "        shutil.rmtree(f\"data/chips/{zarr_id}.zarr\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {zarr_id}: {e}\")\n",
    "\n",
    "\n",
    "# Parallel execution wrapper\n",
    "def process_zarr_ids_parallel(zarr_ids, max_workers=10):\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(clean_zarr, zarr_id): zarr_id for zarr_id in zarr_ids\n",
    "        }\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Processing Zarr files\"\n",
    "        ):\n",
    "            _ = future.result()  # Errors are already printed in `clean_zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54966017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_zarr_ids_parallel(zarr_ids, max_workers=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disturbance-agent-data (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
